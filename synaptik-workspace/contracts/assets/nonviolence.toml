name = "Base Ethics – Nonviolence"
version = "1.0.1"
description = "Block personal threats / violent language; allow benign idioms"

# ----- Allowlist: benign idioms (these win over violations) -----
[[rules]]
action = "say"
effect = "allow"
matches_any = [
  "kill time",
  "killing it",
  "killer app",
  "killer feature",
  "killer deal",
  "kill switch",
  "kill for a slice",
  "kill for a pizza",
  # Broader day-to-day idioms
  "is killing me",                    
  "this is killing me",
  "this debugging session is killing me",
  "this bug is killing me",
  "this task is killing me",
  # Light idioms expressing strong desire
  "kill for a coffee",
  "kill for some coffee",
  "kill for a break",
  "kill for a nap",
  "kill for a better ide",
]
violation = "Benign idiom usage"
severity = "none"
constraints = [
  "clarify_nonliteral_intent",
  "avoid_violent_metaphors_when_possible"
]  # still allow; nudge wording if appropriate

# ----- Personal threats: explicit high-severity (with pronouns or broad groups) -----
[[rules]]
action = "say"
contains_any = [
  # pronoun / person-targeted
  "kill him", "kill her", "kill them", "kill you", "kill me", "kill us",
  "hurt him", "hurt her", "hurt them", "hurt you", "hurt me", "hurt us",
  "attack him", "attack her", "attack them", "attack you", "attack me", "attack us",

  # broad-group patterns
  "kill everyone", "kill every", "kill all", "kill people",
  "hurt everyone", "harm everyone", "attack everyone"
]
violation = "Personal threat or harm language detected"
violation_code = "harm_threat_personal"
action_suggestion = "Please avoid language that suggests harm to people."
severity = "high"
constraints = [
  "refuse_personal_harm_content",
  "avoid_violent_language",
  "reframe_nonviolent",
  "offer_deescalation",
  "do_not_repeat_harmful_phrases"
]

# ----- Violent methods / weapons (also high severity) -----
[[rules]]
action = "say"
contains_any = [
  "stab", "stabbing", "shoot", "shooting", "punch",
  "strangle", "poison", "bomb", "explosive", "behead"
]
violation = "Violent method suggested"
violation_code = "violent_suggestion"
action_suggestion = "I can’t help with violent methods."
severity = "high"
constraints = [
  "refuse_instructions_for_violence",
  "avoid_violent_language",
  "reframe_nonviolent",
  "offer_deescalation",
  "do_not_repeat_harmful_phrases"
]

# ----- Generic harmful / destructive language (lower severity) -----
[[rules]]
action = "say"
contains_any = ["kill", "hurt", "harm", "attack", "destroy", "annihilate", "demolish"]
violation = "Harmful or destructive language"
violation_code = "harm_language_generic"
action_suggestion = "Let’s choose constructive wording."
severity = "medium"
constraints = [
  "soften_language",
  "avoid_violent_language",
  "reframe_constructive"
]

# ----- Allowlist: technical language that uses common verbs benignly -----
[[rules]]
action = "say"
effect = "allow"
matches_any = [
  # Engineering contexts
  "destroy all the legacy code",
  "destroy all legacy code",
  "destroy legacy code",
  "destroy the bug",
  "destroy this bug",
  "kill the bug",
  "kill this bug",
  "kill a process",
  "kill the process",
  "kill -9",
  "send sigkill",
]
violation = "Technical/benign usage allowed"
severity = "none"
constraints = [
  "prefer_precise_technical_terms",
  "clarify_nonliteral_intent"
]

# ----- Justification of violence (high) -----
[[rules]]
action = "say"
contains_any = ["deserved it", "had it coming"]
violation = "Justification of violence detected"
violation_code = "justification_of_violence"
severity = "high"
constraints = [
  "reframe_nonviolent",
  "offer_deescalation",
  "avoid_justification_language"
]
